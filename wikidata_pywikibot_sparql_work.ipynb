{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with pywikibot, Wikidata API, and SPARQL\n",
    "\n",
    "In my reconciliation work, my task is usually to take a string, throw that against an API to search, and evaluate the responses to make the linkage between that string to a URI. This is what's meant by the saying \"from strings to things.\"  \n",
    "\n",
    "A specific use case I've encountered will require a different approach. The use case is: I have already reconciled some strings to VIAF. I then have a list of VIAF IDs. Wikidata is known to have VIAF IDs within its entities. So how can I reconcile the VIAF IDs to Wikidata? \n",
    "\n",
    "There's relatively simple ways to do this assuming I have one VIAF ID. But what if I have thousands? \n",
    "\n",
    "This notebook will first explore the `pywikibot` python program to explore the Wikidata API and see how the data is structured and modeled. We'll then see if we can tackle our problem using SPARQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Working through the pywikibot tutorial \n",
    "\n",
    "Of course we should always start by reading the manual, and helpfully Wikidata has its own tutorial for `pywikibot`. The following section will outline how to explore an item/\"page\". While this isn't our use case, we can start to understand what \"claims\" are. Claims are important because VIAF IDs, which is what we are after, have a specific claim/property: [P214](https://www.wikidata.org/wiki/Property:P214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywikibot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "repo = site.data_repository()\n",
    "item = pywikibot.ItemPage(repo, \"Q76\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_dict = item.get()\n",
    "clm_dict = item_dict[\"claims\"]\n",
    "clm_list = clm_dict[\"P214\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we could just print the claim value to get a FAST ID. But first we should explore the structure of this specific claim in case we'll need that info later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rank': 'preferred', 'references': [{'snaks': {'P143': [{'datavalue': {'value': {'numeric-id': 8447, 'entity-type': 'item'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item', 'snaktype': 'value', 'property': 'P143'}]}, 'snaks-order': ['P143'], 'hash': 'd4bd87b862b12d99d26e86472d44f26858dee639'}], 'mainsnak': {'datavalue': {'value': '52010985', 'type': 'string'}, 'datatype': 'external-id', 'snaktype': 'value', 'property': 'P214'}, 'type': 'statement', 'id': 'q76$9AF526A1-C489-4E26-93E0-B831DE7EC2AD'}\n"
     ]
    }
   ],
   "source": [
    "for clm in clm_list:\n",
    "    print(clm.toJSON())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just print the VIAF ID for Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52010985\n"
     ]
    }
   ],
   "source": [
    "for clm in clm_list:\n",
    "    clm_trgt = clm.getTarget()\n",
    "    print(clm_trgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it checks out. The [Wikidata page](https://www.wikidata.org/wiki/Q76) confirms this is the correct VIAF ID. We're starting to see what we might need for our use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a SPARQL query with a VIAF ID\n",
    "\n",
    "Can we come at this in the opposite way? That is, can we take a VIAF ID and query wikidata via SPARQL? It's actually pretty simple, which should make sense given how SPARQL queries can handle queries with very specific parameters (just like SQL).\n",
    "\n",
    "From the [Wikidata Query Service](https://query.wikidata.org/), we can run the following query:\n",
    "```\n",
    "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?person ?personLabel WHERE {\n",
    "  ?person wdt:P214 \"52010985\"   \n",
    "  SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"en\" .\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The [result](http://tinyurl.com/llsqors) brings back the correct \"Q\" wikidata item/page from above, Q76, with the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did this using Wikidata's own GUI interface. It's convenient, has autocomplete, and infers all kinds of things as you type out the query. But we need to figure out how to do this via python. Let's run the same query using the python library [SPARQLWrapper](https://rdflib.github.io/sparqlwrapper/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import SPARQLWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama\n",
      "http://www.wikidata.org/entity/Q76\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setQuery(\"\"\"    \n",
    "    SELECT ?person ?personLabel\n",
    "    WHERE {\n",
    "  ?person wdt:P214 \"52010985\"   \n",
    "  SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"en\"\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"personLabel\"][\"value\"])\n",
    "    print(result[\"person\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we're looking for. We get the label, and the exact URI for Q76 in Wikidata.  \n",
    "\n",
    "## Iterating a SPARQL query, using a list of values \n",
    "\n",
    "We're getting close to solving our original problem. We have successfully queried Wikidata with a VIAF ID and have found the Wikidata URI it matches. But this is so far a one-by-one operation. We need to be able to run this same query, except we want to substitute in all of our VIAF IDs each time, potentially thousands. Knowing the basics of python, you could probably guess we need a `for` loop. And it would make sense to make our list of VIAF IDs into a list that we can then loop over.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT ?person ?personLabel WHERE { ?person wdt:P214 \"52010985\" SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }}\n",
      "Barack Obama\n",
      "http://www.wikidata.org/entity/Q76\n",
      "SELECT ?person ?personLabel WHERE { ?person wdt:P214 \"34562701\" SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }}\n",
      "Herbert York\n",
      "http://www.wikidata.org/entity/Q1609351\n",
      "SELECT ?person ?personLabel WHERE { ?person wdt:P214 \"108815043\" SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }}\n",
      "SELECT ?person ?personLabel WHERE { ?person wdt:P214 \"76323201\" SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }}\n",
      "Avrum Stroll\n",
      "http://www.wikidata.org/entity/Q4829518\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "viaf_id = [\"52010985\", \"34562701\", \"108815043\", \"76323201\"]\n",
    "\n",
    "\n",
    "for f in viaf_id:\n",
    "    queryString = 'SELECT ?person ?personLabel WHERE { ?person wdt:P214 \"' + f + '\" SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    result1 = sparql.query().convert()\n",
    "    print(queryString)\n",
    "    for result in result1[\"results\"][\"bindings\"]:\n",
    "        print(result[\"personLabel\"][\"value\"])\n",
    "        print(result[\"person\"][\"value\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
